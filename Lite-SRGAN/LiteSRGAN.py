import random
import cv2
import numpy as np
import glob
import os
import tensorflow as tf
import keras
import matplotlib.pyplot as plt
from keras import backend as K
from keras.applications.mobilenet_v2 import MobileNetV2
from VGG19 import VGG19,preprocess_input
from keras.models import Model
from keras.layers import Conv2D,MaxPooling2D,Activation, BatchNormalization,Flatten,Dense,Dropout,InputLayer,DepthwiseConv2D , Add ,  UpSampling2D, Input, Concatenate
from keras.models import Sequential
from keras.losses import BinaryCrossentropy,MeanSquaredError,MeanAbsoluteError
from tensorflow.keras.optimizers.schedules import PiecewiseConstantDecay
from keras import optimizers


class LiteSRGAN():
    """ LiteSRGAN for high quality real time super-resolution """

    def __init__(self, args):

        self.upsamplingblocks = args.upsampling_blocks
        self.generator_weights = args.generator_weights
        self.lr=args.lr
        self.decay_steps=args.decay_steps
        self.decay_rate=args.decay_rate
        self.img_height=args.img_height
        self.img_width=args.img_width
        self.discriminator_weights=args.discriminator_weights
        self.generator_weights=args.generator_weights
        # initial number of filters for generator and discriminator
        self.gen_filters = 32
        self.disc_filters = 8
        # number of generator residual blocks
        self.gen_residual_blocks = 12 


        # Define a learning rate decay schedule for both generator and discriminator.
        self.gen_lr_schedule = PiecewiseConstantDecay(boundaries=[self.decay_steps], values=[self.lr, self.lr*self.decay_rate])  
        self.disc_lr_schedule = PiecewiseConstantDecay(boundaries=[ self.decay_steps], values=[self.lr, self.lr*self.decay_rate])

        self.gen_optimizer = tf.keras.optimizers.Adam(learning_rate=self.gen_lr_schedule)
        self.disc_optimizer = tf.keras.optimizers.Adam(learning_rate=self.disc_lr_schedule)

        # build VGG19 network that outputs high level features suitable for computing perceptual loss
        self.vgg_perceptual = self.build_vgg(loss="perceptual")
        
        #build VGG19 network that outputs the features from multiple layers (low and high level features suitable for computing style/texture loss)
        self.vgg_style=self.build_vgg(loss="style")
        
        # if you want to use the features from mobilenet instead of vgg for perceptual_loss
        self.mobileNet = self.build_mobilenet()

        # discriminator downsampling factor (4 strided convolution)
        disc_downsampling_factor= 2 ** 4

        #verify that the image width and height is larger than the discrimiantor downsampling factor used
        patch_h = int(self.img_height / disc_downsampling_factor)
        patch_w= int(self.img_width / disc_downsampling_factor)


        assert patch_h >= 1 , f"The provided image height is expected to be greater than {disc_downsampling_factor}"
        assert patch_w >= 1 , f"The provided image width is expected to be greater than {disc_downsampling_factor}"
        self.disc_patch = (patch_h, patch_w, 1)

        # build the generator and discriminator models
        self.discriminator = self.build_discriminator()
        self.generator = self.build_generator()


    @tf.function
    def PixelLoss(self,hr,sr, criterion="l1"):
        """
        :param hr: HR image.
        :param sr: SR image generated by the generator network.
        :param criterion: whether to apply l1 or l2 for calculating Pixel-wise loss.
        :return: returns the pixel loss.
        """
        if criterion.lower() == "l1":
            return MeanAbsoluteError()(hr,sr)
        elif criterion.lower() == "l2":
            return MeanSquaredError()(hr,sr)
        else:
            raise NotImplementedError(
                'The provided loss type {} is not recognized , Hint: please choose either l1 or l2'.format(criterion)
            )

    def build_vgg(self,loss="perceptual"):
        """
        build a pre-trained VGG19 network that output image features.
        based on the given loss type.
        :param loss: holds the type of loss to be computed.
        :return: returns the VGG19 model with the specified output layers suitable for computing the given loss type.
        """

        vgg = VGG19(weights="imagenet", input_shape=(self.img_height, self.img_width, 3), include_top=False)
        vgg.trainable = False

        if loss.lower()=="perceptual":
            # Get the vgg network. Extract features from Block 5, last convolution.
            # Note: we extract the last layer before activation instead of after it to produce better textures and visual results as proposed by Xintao Wang et al.
            model = keras.models.Model(inputs=vgg.input, outputs=vgg.get_layer("block5_conv4_before_activation").output)
        elif loss.lower()=="style":
            #style loss is computed based on different low/high level features from VGG19 network where it tries to preserve the texture,color as well as the content between the original image and the generated one
            model = keras.models.Model(inputs=vgg.input, outputs=[vgg.get_layer("block1_conv1").output,
                                                                  vgg.get_layer("block2_conv1").output,
                                                                  vgg.get_layer("block3_conv1").output,
                                                                  vgg.get_layer("block4_conv1").output,
                                                                  vgg.get_layer("block5_conv1").output])
        return model

    @tf.function
    def PerceptualLoss(self, hr, sr, criterion="l2"):
        """
        :param hr: HR image.
        :param sr: SR image.
        :param criterion: whether to apply l1 or l2 for calculating perceptual loss.
        :return: returns the perceptual loss.
        """
        hr = preprocess_input(((hr + 1.0) * 255) / 2.0)
        sr = preprocess_input(((sr + 1.0) * 255) / 2.0)
        hr_features = self.vgg_perceptual(hr)
        sr_features = self.vgg_perceptual(sr)

        hr_features = hr_features / 12.75
        sr_features = sr_features / 12.75
        if criterion.lower() == "l1":
            return MeanAbsoluteError()(hr_features, sr_features)
        elif criterion.lower() == "l2":
            return MeanSquaredError()(hr_features, sr_features)
        else:
            raise NotImplementedError(
                'The provided criterion {} is not recognized for computing perceptual loss , Hint: please choose either l1 or l2'.format(
                    criterion)
            )

    def gram_matrix(self, input):
        '''
        :param input: takes the 4d tensor which corresponds to the output specified layer of the vgg network.
        :return: returns the gram_matrix (correlation matrix) = (M.MTranspose).
        '''
        assert K.ndim(input) == 4, 'Input tensor should be 4d (batch_size,H,W,Channels)'
        assert K.image_data_format() == 'channels_last', "Please use your batch as the first dim and the channels as your last dim"

        input = K.permute_dimensions(input, (0, 3, 1, 2))
        tensor_shape = K.shape(input)
        B, C, H, W = tensor_shape[0], tensor_shape[1], tensor_shape[2], tensor_shape[3]

        # Reshape the input and do batch dot product
        matrix = K.reshape(input, K.stack([B, C, H * W]))

        gram_matrix = K.batch_dot(matrix, matrix, axes=2)

        # normalize the gram_matrix with Height , Width and Channels
        gram_matrix = gram_matrix / K.cast(H * W * C, input.dtype)
        return gram_matrix

    @tf.function
    def StyleLoss(self,hr, sr, criterion="l2"):
        """
        :param hr: HR image.
        :param sr: SR image.
        :param criterion: whether to apply l1 or l2 for calculating style loss.
        :return: returns the style loss.
        """
        hr = preprocess_input(((hr + 1.0) * 255) / 2.0)
        sr = preprocess_input(((sr + 1.0) * 255) / 2.0)
        hr_features = self.vgg_style(hr)
        sr_features = self.vgg_style(sr)

        style_loss=0
        if criterion.lower() == "l1":
            for f1 , f2 in zip(hr_features,sr_features):
                style_loss+= MeanAbsoluteError()(self.gram_matrix(f1), self.gram_matrix(f2))
        elif criterion.lower() == "l2":
            for f1, f2 in zip(hr_features, sr_features):
                style_loss += MeanSquaredError()(self.gram_matrix(f1), self.gram_matrix(f2))
        else:
            raise NotImplementedError(
                'The provided criterion {} is not recognized for computing style loss , Hint: please choose either l1 or l2'.format(
                    criterion)
            )
        return style_loss
    
    @tf.function
    def AdversarialLoss(self, sr_prediction):
        """
        :param sr_prediction: discriminator network prediction when given Fake (SR image) generated by the generator network.
        :return: returns the adversarial loss.
        """
        adv_loss=BinaryCrossentropy()(tf.ones_like(sr_prediction),sr_prediction)
        return adv_loss

    @tf.function
    def DiscriminatorLoss(self, hr_prediction,sr_prediction):
        """
        :param hr_prediction: discriminator network prediction when given HR image.
        :param sr_prediction: discriminator network prediction when given Fake (SR image).
        :return: returns the discriminator loss.
        """
        hr_loss = BinaryCrossentropy()(tf.ones_like(hr_prediction), hr_prediction)
        sr_loss = BinaryCrossentropy()(tf.zeros_like(sr_prediction), sr_prediction)
        disc_loss= tf.add(hr_loss,sr_loss)
        return disc_loss


    def build_mobilenet(self):
        """
        Builds a pre-trained mobilenet model that outputs a feature map based on
        the layer specified you can use it as a backbone for perceptual loss for faster training
        instead of vgg19 but vgg19 gets somehow better results, also you can adjust the output
        layer of MobileNetV2 to output different the feature map.
        :return: returns the MobileNetV2 model with the specified output layer suitable for computing perceptual loss.
        """

        mobileNet = MobileNetV2(weights="imagenet", input_shape=(self.img_height, self.img_width, 3), include_top=False)
        mobileNet.trainable = False
        for layer in mobileNet.layers:
            layer.trainable = False

        # Create model and compile
        model = keras.models.Model(inputs=mobileNet.input,
                                   outputs=mobileNet.get_layer("block_15_depthwise_relu").output)

        return model


    def build_generator(self):
        """Build the lightweight generator based on the inverted residual blocks
        of MobileNetV2."""

        def _inverted_res_block(inputs, expansion, stride, alpha, filters, block_id):
            """
            Inverted Residual block that uses depth-wise convolution for obtaining an optimized network in terms of complexity and latency.
            :param inputs: The input feature map.
            :param expansion: Channel expansion factor.
            :param stride: Convolution stride
            :param alpha: Depth expansion factor.
            :param filters: Number of filters in each conv layer.
            :param block_id: An integer specifier for the id of the block in the graph.
            :return: The output of the inverted residual block.
            """
            channel_axis = 1 if keras.backend.image_data_format() == 'channels_first' else -1

            in_channels = keras.backend.int_shape(inputs)[channel_axis]
            pointwise_conv_filters = int(filters * alpha)
            #Ensure the number of filters on the last 1x1 convolution is divisible by 8
            pointwise_filters = _make_divisible(pointwise_conv_filters, 8)
            x = inputs
            prefix = 'block_{}_'.format(block_id)

            if block_id:
                # Expand with a pointwise 1x1 convolution (Expansion convolution)
                x = keras.layers.Conv2D(expansion * in_channels,
                                        kernel_size=1,
                                        padding='same',
                                        use_bias=True,
                                        activation=None,
                                        name=prefix + 'expand')(x)
                x = keras.layers.BatchNormalization(axis=channel_axis,
                                                    epsilon=1e-3,
                                                    momentum=0.999,
                                                    name=prefix + 'expand_BN')(x)
                x = keras.layers.Activation('relu', name=prefix + 'expand_relu')(x)
            else:
                prefix = 'expanded_conv_'

            # Depthwise 3x3 conv
            x = keras.layers.DepthwiseConv2D(kernel_size=3,
                                             strides=stride,
                                             activation=None,
                                             use_bias=True,
                                             padding='same' if stride == 1 else 'valid',
                                             name=prefix + 'depthwise')(x)
            x = keras.layers.BatchNormalization(axis=channel_axis,
                                                epsilon=1e-3,
                                                momentum=0.999,
                                                name=prefix + 'depthwise_BN')(x)

            x = keras.layers.Activation('relu', name=prefix + 'depthwise_relu')(x)

            #  Project with a pointwise 1x1 convolution (Projection convolution)
            x = keras.layers.Conv2D(pointwise_filters,
                                    kernel_size=1,
                                    padding='same',
                                    use_bias=True,
                                    activation=None,
                                    name=prefix + 'project')(x)
            x = keras.layers.BatchNormalization(axis=channel_axis,
                                                epsilon=1e-3,
                                                momentum=0.999,
                                                name=prefix + 'project_BN')(x)

            if in_channels == pointwise_filters and stride == 1:
                return keras.layers.Add(name=prefix + 'add')([inputs, x])
            return x


        def _upsample_block(input):
            """
            upsample the input with a factor of 2 (2x upscaling).
            :param input: The given input lr tensor to upsample.
            :return u: 2x upscaled input.
            """

            u = keras.layers.UpSampling2D(size=2, interpolation='bilinear')(input)
            u = keras.layers.Conv2D(self.gen_filters, kernel_size=3, strides=1, padding='same')(u)
            u = keras.layers.PReLU(shared_axes=[1, 2])(u)
            return u

        def _make_divisible(v, divisor, min_value=None):
            """
            It ensures that all layers have a channel number divisible by 8
            """
            if min_value is None:
                min_value = divisor
            new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)
            # Make sure that round down does not go down by more than 10%.
            if new_v < 0.9 * v:
                new_v += divisor
            return new_v
        
        # Low resolution image input
        input_lr = keras.Input(
            shape=(int(self.img_height // (2**self.upsamplingblocks)), int(self.img_width // (2**self.upsamplingblocks)), 3))

        # generator first block (pre-inverted residual blocks)
        c1 = keras.layers.Conv2D(self.gen_filters, kernel_size=3, strides=1, padding='same')(input_lr)
        c1 = keras.layers.BatchNormalization()(c1)
        c1 = keras.layers.PReLU(shared_axes=[1, 2])(c1)

        # propagate through the inverted residual blocks
        r = _inverted_res_block(inputs=c1,expansion=6,stride=1,alpha=1.0,filters=self.gen_filters, block_id=0)
        for idx in range(1, self.gen_residual_blocks):
            r = _inverted_res_block(inputs=r, expansion=6,stride=1,alpha=1.0,filters=self.gen_filters,block_id=idx)

        # post-inverted residual blocks
        c2 = keras.layers.Conv2D(self.gen_filters, kernel_size=3, strides=1, padding='same')(r)
        c2 = keras.layers.BatchNormalization()(c2)
        c2 = keras.layers.Add()([c2, c1])

        generator_model = None
        if self.upsamplingblocks>=1:

            # first upsample block
            u = _upsample_block(c2)
            # propagate through the remaining upsample blocks
            for i in range(self.upsamplingblocks-1):
                u= _upsample_block(u)
            out_hr = keras.layers.Conv2D(3, kernel_size=3, strides=1, padding='same', activation='tanh')(u)
            generator_model = keras.models.Model(input_lr, out_hr)
            if self.generator_weights:
                generator_model.load_weights(self.generator_weights)

        else:
            raise ValueError(
                '''The number of upsampling blocks must be >=1,
                   as the minimum possible upscaling is 2x '''
            )

        return generator_model

    def build_discriminator(self):
        """Builds a Lightweight discriminator by replacing normal convolution with separable depthwise convolution."""
        
        
        def d_block(layer_input, filters, strides=1, bn=True):
            """
            Discriminator block
            :param layer_input: Input feature map.
            :param filters: Convolution filters.
            :param strides: Convolution stride.
            :param bn: whether to apply batch normalization after convolution or not.
            :return: output of the discriminator block
            """

            d = keras.layers.DepthwiseConv2D(kernel_size=3, strides=strides, padding='same')(layer_input)
            d= keras.layers.Conv2D(kernel_size=1,filters=filters)(d)
            if bn:
                d = keras.layers.BatchNormalization(momentum=0.8)(d)
            d = keras.layers.LeakyReLU(alpha=0.2)(d)

            return d
        # Input img
        d0 = keras.layers.Input(shape=(self.img_height, self.img_width, 3))

        d1 = d_block(d0, self.disc_filters, bn=False)
        d2 = d_block(d1, self.disc_filters, strides=2)
        d3 = d_block(d2, self.disc_filters * 2)
        d4 = d_block(d3, self.disc_filters * 2, strides=2)
        d5 = d_block(d4, self.disc_filters * 4)
        d6 = d_block(d5, self.disc_filters * 4, strides=2)
        d7 = d_block(d6, self.disc_filters * 8)
        d8 = d_block(d7, self.disc_filters * 8, strides=2)

        d9 = Dense(self.disc_filters * 16)(d8)
        d10 = keras.layers.LeakyReLU(0.2)(d9)

        validity = Dense(1, activation='sigmoid')(d10)
        print("last layer shape of discriminator: ", np.shape(validity))
        discriminator_model = keras.models.Model(d0, validity)

        if self.discriminator_weights != None:
            discriminator_model.load_weights(self.discriminator_weights)

        return discriminator_model

class LiteSRGAN_engine():
    def __init__(self,args,LiteSRGAN_obj):
        self.img_height=args.img_height
        self.img_width=args.img_width
        self.upsamplingblocks=args.upsampling_blocks
        self.images_dir = args.images_dir
        self.images = sorted(glob.glob(os.path.join(self.images_dir, r"**/*.*"),
                                       recursive=True))  ## Read images with any extension (jpg or JPG or jpeg or png)
        self.model= LiteSRGAN_obj
        random.Random(10).shuffle(self.images) 


    def generator_pretraining(self,data,epoch_num):
        """
        Pretraining the generator network with Pixel loss only in the initial iterations to avoid undesired local optima.
        :param data: iterator object that yields batches from the LR and HR images when iterated over.
        :param epoch_num: current epoch number
        :return: None.
        """
        
        print("------------- pre-training epoch {} -------------".format(epoch_num))
        pretrain_iterations = 0
        for lr, hr, last_batch in data:
            print("iteration num = {}".format(pretrain_iterations))
            with tf.GradientTape() as tape:
                sr = self.model.generator(lr)
                pixel_loss = self.model.PixelLoss(hr, sr, criterion="l2")
                
            grads = tape.gradient(pixel_loss, self.model.generator.trainable_variables)
            self.model.gen_optimizer.apply_gradients(zip(grads, self.model.generator.trainable_variables))
            
            if last_batch == True:  ## if removed will lead to infinite loop as we loop on a generator object
                break

            pretrain_iterations += 1

    @tf.function
    def train_on_batch(self, lr, hr): #update step
        """
        Update step for adjusting the weights of both generator and discriminator networks.
        :param lr: Low-resolution image.
        :param hr: High-resolution image.
        :return: returns the generator losses and discriminator loss.
        """

        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
            # generated super-resolved (SR) image when passing LR image to the generator network
            sr = self.model.generator(lr)

            disc_hr_prediction = self.model.discriminator(hr)  # Shape :(batch size,16,16,1)
            disc_sr_prediction = self.model.discriminator(sr)  # Shape :(batch size,16,16,1)

            # The weight for each loss can be tuned for controlling the contribution of each loss to the total loss function
            perceptual_loss = 0.065 * self.model.PerceptualLoss(hr, sr,criterion="l2")
            adv_loss = 1e-3 * self.model.AdversarialLoss(disc_sr_prediction)
            pixel_loss = 0.2 * self.model.PixelLoss(hr, sr,criterion="l1")
            style_loss = 2e-7 * self.model.StyleLoss(hr, sr,criterion="l2")


            total_loss = perceptual_loss + adv_loss + pixel_loss + style_loss

            # Discriminator loss
            disc_loss=self.model.DiscriminatorLoss(disc_hr_prediction,disc_sr_prediction)

        # calculate the gradients and update the generator network weights
        gen_grads = gen_tape.gradient(total_loss, self.model.generator.trainable_variables)
        self.model.gen_optimizer.apply_gradients(zip(gen_grads, self.model.generator.trainable_variables))

        # calculate the gradients and update the discriminator network weights
        disc_grads = disc_tape.gradient(disc_loss, self.model.discriminator.trainable_variables)
        self.model.disc_optimizer.apply_gradients(zip(disc_grads, self.model.discriminator.trainable_variables))

        return disc_loss, adv_loss, perceptual_loss, pixel_loss, style_loss

    def train(self, data, verboseIter, epoch):
        """
        Training the generator and discriminator networks.
        :param data: iterator object that yields batches from the LR and HR images when iterated over.
        :param verboseIter: the iteration in which the loss values are printed.
        :param epoch: number of current epoch.
        :return: None.
        """

        disc_loss_list = []
        adv_loss_list = []
        perceptual_loss_list = []
        pixel_loss_list = []
        style_loss_list = []
        train_iterations=0
        # loop over the data generator
        for lr, hr, last_batch in data:

            disc_loss, adv_loss, perceptual_loss, pixel_loss, style_loss = self.train_on_batch( lr, hr)

            disc_loss_list.append(disc_loss.numpy())
            adv_loss_list.append(adv_loss.numpy())
            perceptual_loss_list.append(perceptual_loss.numpy())
            pixel_loss_list.append(pixel_loss.numpy())
            style_loss_list.append(style_loss.numpy())
            # print the value of each loss function when the verboseIter is reached
            if train_iterations % verboseIter == 0:

                print("[EP{}] ".format(epoch) + "Adversarial Loss= " + str(adv_loss.numpy()) \
                      + " Perceptual Loss= " + str(perceptual_loss.numpy()) \
                      + " Pixel Loss= " + str(pixel_loss.numpy()) \
                      + " Discriminator Loss= " + str(disc_loss.numpy()) \
                      + " Style Loss= " + str(style_loss.numpy()))
            
            # save the generator and discriminator models at the end of each epoch
            if last_batch == True:
                self.model.generator.save('models/generator.h5')
                self.model.discriminator.save('models/discriminator.h5')


                print("After Epoch {} -------------------------------------------------------".format(epoch))
                print("Adversarial Loss= " + str(np.mean(adv_loss_list)) \
                      + " Perceptual Loss= " + str(np.mean(perceptual_loss_list)) \
                      + " Pixel Loss= " + str(np.mean(pixel_loss_list)) \
                      + " Discriminator Loss= " + str(np.mean(disc_loss_list)) \
                      + " Style Loss= " + str(np.mean(style_loss_list)))

                disc_loss_list.clear()
                adv_loss_list.clear()
                perceptual_loss_list.clear()
                mse_loss_list.clear()
                style_loss_list.clear()

                break
            train_iterations += 1  # increment after each batch

    def saveTrails(self,n_samples,epoch):
        """
        save some random trails to keep track of the generated images after each epoch .
        :param n_samples: number of random samples to be saved.
        :param epoch: number of the current epoch.
        :return: None.
        """
        random_samples=self.images[:n_samples]
        for index , img in enumerate(random_samples):
            low_res = cv2.imread(img, 1)
            # Convert to RGB (opencv uses BGR as default)
            low_res = cv2.cvtColor(low_res, cv2.COLOR_BGR2RGB)
            low_res = cv2.resize(low_res, [self.img_width // (2 ** self.upsamplingblocks), self.img_height // (2 ** self.upsamplingblocks )])
            # Rescale to 0-1.
            low_res = low_res / 255.0

            # Get super resolution image
            sr = self.model.generator.predict(np.expand_dims(low_res, axis=0))[0]

            # Rescale values in range 0-255
            sr = (((sr + 1) / 2.) * 255).astype(np.uint8)

            plt.imsave("generatedTrails/{}_".format(index) + str(epoch) + '.png', sr)



